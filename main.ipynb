{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd9558f",
   "metadata": {},
   "source": [
    "<!-- # Overview\n",
    "- Project\n",
    "    - data engineer \n",
    "        - keep acceptable sessions\n",
    "        - create prediction classes and eliminate end of each session\n",
    "        - make same input size for rnn\n",
    "        - add features if this is suitable\n",
    "    - code each rnn\n",
    "        - vanilla rnn\n",
    "        - lstm\n",
    "        - gru -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9df30d",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Load Data\n",
    "    - \n",
    "- Modelling\n",
    "    - Train-Test-Split\n",
    "    - RNN\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - LSTM\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - GRU\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - (Transformer Model Def)\n",
    "        - Transfer Learning/Model Def\n",
    "        - Model Training\n",
    "        \n",
    "- Evaluation and Comparison of Models\n",
    "    - Numerical Evalutaion Metrics\n",
    "        - AUC and F1 Score\n",
    "       - (Plotting number results comparing each as parameters change)\n",
    "    - Plotted Evaluation Metrics\n",
    "        - ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d865010",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8176e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Evaluation\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics.functional import auc\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics.functional import auc\n",
    "from torchmetrics import ROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# handling time data\n",
    "import time # for timestamps\n",
    "import datetime\n",
    "\n",
    "# data analysis\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56f934",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655bcdb",
   "metadata": {},
   "source": [
    "### Baseline - The baseline would be predicting that nobody will purchase\n",
    "## THIS SHOULD BE PUT LATER ACTUALLY ONCE CLASSES ARE MADE MORE BALANCED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2692d94",
   "metadata": {},
   "source": [
    "#### For 2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc8c2b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9587844058150812"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(my2Labels, return_counts = True)[1][0] / len(my2Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8d845",
   "metadata": {},
   "source": [
    "#### For 3 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d6fd1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106947870051741"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(my3Labels, return_counts = True)[1][0] / len(my2Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ef443d",
   "metadata": {},
   "source": [
    "####\n",
    "- https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "\n",
    "- There's one additional rule of thumb that helps for supervised learning problems. You can usually prevent over-fitting if you keep your number of neurons below:\n",
    "\n",
    "𝑁ℎ=𝑁𝑠(𝛼∗(𝑁𝑖+𝑁𝑜))\n",
    "\n",
    "- 𝑁𝑖 = number of input neurons.\n",
    "- 𝑁𝑜 = number of output neurons.\n",
    "- 𝑁𝑠 = number of samples in training data set.\n",
    "- 𝛼 = an arbitrary scaling factor usually 2-10.\n",
    "\n",
    "Guy says he geneerally uses 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04000357",
   "metadata": {},
   "source": [
    "#### https://www.reddit.com/r/MachineLearning/comments/4behuh/does_the_number_of_layers_in_an_lstm_network/\n",
    "Some discussion about what \"depth\" in recurrent architectures means. Downward skip-connections seem to be the most helpful, but in general skip connections are critical in deep recurrent networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c464fe3",
   "metadata": {},
   "source": [
    "#### \n",
    "- https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "- usually one hidden layer is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f9121",
   "metadata": {},
   "source": [
    "### This looks like best tutorial to follow so far (in an article)\n",
    "- https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "- https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks\n",
    "    - didn't look bad either\n",
    "- https://docs.wandb.ai/guides/integrations/pytorch\n",
    "    - logging gradients with wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b305bb5",
   "metadata": {},
   "source": [
    "### Youtube Tutorial - Just Like Best One up There\n",
    "- https://www.youtube.com/watch?v=1vGOQAel2yU&ab_channel=SungKim\n",
    "- packed sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84b92f",
   "metadata": {},
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eca39",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_class_metrics(pred, target, positive_class = 1):\n",
    "    conf_mat = ConfusionMatrix(num_classes=2)\n",
    "    cm = conf_mat(pred, target)\n",
    "    print(\"Confusion Matrix (0 in Top Left): \")\n",
    "    print(cm)\n",
    "    \n",
    "    if positive_class == 1: \n",
    "        # true positives / (true positives + false positives)\n",
    "        # recall = true positives / (true positives + false negatives)\n",
    "        tp = cm[1][1]\n",
    "        fp = cm[0][1]\n",
    "        fn = cm[1][0]\n",
    "    else:\n",
    "        tp = cm[0][0]\n",
    "        fp = cm[1][0]\n",
    "        fn = cm[0][1]\n",
    "      \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    if not ((precision.item() > 0) & (precision.item() < 100)):\n",
    "        precision = 0\n",
    "        print(\"\\n\\nprecision is nan (has been set to 0)\")\n",
    "    \n",
    "    if not ((recall.item() > 0) & (recall.item() < 100)):\n",
    "        recall = 0\n",
    "        print(\"recall is nan (has been set to 0)\")\n",
    "    \n",
    "    if not ((f1score.item() > 0) & (f1score.item() < 100)):\n",
    "        f1score = 0\n",
    "        print(\"F1score is nan (has been set to 0)\\n\\n\")\n",
    "    \n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1-Score: \", f1score)\n",
    "    \n",
    "#     roc = ROC(num_classes = 2, pos_label=1)\n",
    "#     score_fpr, score_tpr, _ = roc(target, pred)\n",
    "#     score_roc_auc = roc_auc_score(target, pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a714957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_metrics(pred, target, num_classes):\n",
    "    conf_mat = ConfusionMatrix(num_classes=num_classes)\n",
    "    cm = conf_mat(pred, target)\n",
    "    print(\"Confusion Matrix (0 in Top Left): \")\n",
    "    print(cm)\n",
    "    \n",
    "    \n",
    "    metric = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "    f1_score_avg = metric(pred, target)\n",
    "    print(\"F1-Score (Average)\", f1_score_avg)\n",
    "\n",
    "    metric = MulticlassF1Score(num_classes=num_classes, average=None)\n",
    "    f1_score_each = metric(pred, target)\n",
    "    print(\"F1-Score (each):\")\n",
    "    for i, f in enumerate(f1_score_each):\n",
    "        print(\"Class \", i, \":\", f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000c3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(model, num_class, dataloader):\n",
    "    # to store all labels and predictions for f1-score\n",
    "    all_pred = []\n",
    "    all_label = []\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs, h = model(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_pred.append(predicted)\n",
    "            all_label.append(labels)\n",
    "\n",
    "    # get all predictions and labels into one array and as integer tensors\n",
    "    all_pred = [i for s in all_pred for i in s]\n",
    "    all_label = [i for s in all_label for i in s]\n",
    "    all_label = [all_label[i][0] for i in range(0, len(all_label))]\n",
    "    all_label = [all_label[i].to(dtype=torch.long) for i in range(len(all_label))]\n",
    "\n",
    "    all_pred = torch.LongTensor(all_pred)\n",
    "    all_label = torch.LongTensor(all_label)\n",
    "    if num_class == 2:\n",
    "        bin_class_metrics(all_pred, all_label, positive_class = 1)\n",
    "    elif num_class > 2:\n",
    "        multiclass_metrics(all_pred, all_label, num_class)\n",
    "    return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282bbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, model_name, num_classes, train_dl, test_dl):\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(model_name, \" Metrics\")\n",
    "    print(\"Train\")\n",
    "    preds_lstm_train = evaluate_model_metrics(model, num_classes, train_dl)\n",
    "\n",
    "    print(\"Test\")\n",
    "    preds_lstm = evaluate_model_metrics(lstm, num_classes, test_dl)\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1882",
   "metadata": {},
   "source": [
    "# Model Definitions\n",
    "#### Training Function\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
    "    - training with validation as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb18277",
   "metadata": {},
   "source": [
    "## Vanilla RNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53df8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#         print(\"hidden2 shape: \", hidden.shape)\n",
    "#         print(\"output1 shape: \", out.shape)\n",
    "       \n",
    "        # out = out[:, -1]   # found from online for a certain error, might want to investigate further\n",
    "                            # refer to this for fix -> i think about three entries down\n",
    "                            # https://stackoverflow.com/questions/4493554/neural-network-always-produces-same-similar-outputs-for-any-input\n",
    "       \n",
    "        # out = out[:, -1, :] # this solution from https://discuss.pytorch.org/t/cross-entropy-loss-target-size-and-output-size-mismatch/99031/8\n",
    "                            # makes the loss huuuuggee\n",
    "        \n",
    "        # https://discuss.pytorch.org/t/valueerror-expected-target-size-32-7-got-torch-size-32/42409/4\n",
    "            # might have some useful information about this stuff\n",
    "        \n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.hidden_size) # from second resource below\n",
    "                                                          # doesn't seem to work either\n",
    "#         print(\"out1 reshaped: \", out.reshape(-1, hidden_size))  # also doesn't seem to work\n",
    "#         out = self.linear(out.reshape(-1, hidden_size))\n",
    "        #scores = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea403184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers = 1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        # self.embedding = (input_size, hidden_size)\n",
    "        \n",
    "        # input_size might need to be hidden_size as well\n",
    "        #nonlinearity='relu',\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size,  batch_first = True, dropout = 0)\n",
    "        # MAYBE NEED TO ADD ANOTHER LINEAR LAYER\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "\n",
    "        batch_size = sequences.size(0)\n",
    "    \n",
    "        #embedded = self.embedding(sequence)\n",
    "#         print(sequences.shape)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(sequences, hidden) # embedded here for sequence if not commented out\n",
    "                \n",
    "        out = self.linear(hidden[-1])\n",
    "    \n",
    "        return out, hidden\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e55ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hunkim/PyTorchZeroToAll/blob/master/13_2_rnn_classification.py\n",
    "# - written with this, use the one below\n",
    "# https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "\n",
    "# TRY THIS - THIS ONE WORKED!!!\n",
    "# https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L15/1_lstm.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d02e0",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "- can try stacking lstm\n",
    "- dropout\n",
    "- bidirectional lstm (isn't really used for predicting the future)\n",
    "- cnn lstm hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d769d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers = 1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        # self.embedding = (input_size, hidden_size)\n",
    "        \n",
    "        # input_size might need to be hidden_size as well\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first = True, dropout = 0)\n",
    "        # MAYBE NEED TO ADD ANOTHER LINEAR LAYER\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "\n",
    "        batch_size = sequences.size(0)\n",
    "    \n",
    "        #embedded = self.embedding(sequence)\n",
    "#         print(sequences.shape)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        cell = self._init_hidden(batch_size)\n",
    "        out, (hidden, cell) = self.lstm(sequences, (hidden, cell)) # embedded here for sequence if not commented out\n",
    "    \n",
    "#         output, hidden = self.lstm(sequences)\n",
    "        out = self.linear(hidden[-1])\n",
    "    \n",
    "        return out, hidden\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efd6af",
   "metadata": {},
   "source": [
    "## GRU Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72caed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers = 1):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        # self.embedding = (input_size, hidden_size)\n",
    "        \n",
    "        # input_size might need to be hidden_size as well\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, batch_first = True, dropout = 0)\n",
    "        # MAYBE NEED TO ADD ANOTHER LINEAR LAYER\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "\n",
    "        batch_size = sequences.size(0)\n",
    "    \n",
    "        #embedded = self.embedding(sequence)\n",
    "#         print(sequences.shape)\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        out, hidden = self.gru(sequences, hidden) # embedded here for sequence if not commented out\n",
    "            \n",
    "        out = self.linear(hidden[-1])\n",
    "    \n",
    "        return out, hidden\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a546b",
   "metadata": {},
   "source": [
    "## Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89033d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, num_epochs, train_dataloader, val_dataloader):\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            #labels = labels.long() # convert to expected target datatype (Long which is equivalent to int here)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, h = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs,labels.view(-1).long()) # do i need to fix what's in here (even necessary to have it)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf9973",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8a9d2",
   "metadata": {},
   "source": [
    "#### Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e799650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloders for 2 classes\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader_2 \n",
    "test_dataloader_2 = get_dataloaders(2, batch_size)\n",
    "\n",
    "train_dataloader_3 \n",
    "test_dataloader_3 = get_dataloaders(3, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd12754",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7454297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460c946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54724b14",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b6772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1dd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fbfc751",
   "metadata": {},
   "source": [
    "#### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b413350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters for instantiation\n",
    "input_size = seq_arrays.shape[2]\n",
    "hidden_size = 32\n",
    "output_size = 2\n",
    "n_layers = 1\n",
    "\n",
    "#instantiate model\n",
    "rnn = RNNClassifier(input_size, hidden_size, output_size, n_layers)\n",
    "\n",
    "loss_fn_rnn = torch.nn.CrossEntropyLoss() # SHOULD USE WEIGHT PARAMETER SINCE UNBALANCED\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "num_epochs = 3\n",
    "rnn = train_model(rnn, loss_fn_rnn, optimizer_rnn, num_epochs, train_dataloader_2, val_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f78f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"RNN Metrics\")\n",
    "print(\"Train\")\n",
    "preds_rnn_train = evaluate_model_metrics(rnn, 2, train_dataloader_2)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_rnn = evaluate_model_metrics(rnn, 2, test_dataloader_2)\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc8f4a",
   "metadata": {},
   "source": [
    "#### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84101b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters for instantiation\n",
    "input_size = seq_arrays.shape[2]\n",
    "hidden_size = 32\n",
    "output_size = 2\n",
    "n_layers = 1\n",
    "\n",
    "#instantiate model\n",
    "lstm = LSTMClassifier(input_size, hidden_size, output_size, n_layers)\n",
    "\n",
    "loss_fn_lstm = torch.nn.CrossEntropyLoss() # SHOULD USE WEIGHT PARAMETER SINCE UNBALANCED\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_lstm = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe15fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "num_epochs = 3\n",
    "lstm = train_model(lstm, loss_fn_lstm, optimizer_lstm, num_epochs, train_dataloader_2, val_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"LSTM Metrics\")\n",
    "print(\"Train\")\n",
    "preds_lstm_train = evaluate_model_metrics(lstm, 2, train_dataloader_2)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_lstm = evaluate_model_metrics(lstm, 2)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56081",
   "metadata": {},
   "source": [
    "#### Train GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "96a48919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters for instantiation\n",
    "input_size = seq_arrays.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "n_layers = 1\n",
    "\n",
    "#instantiate model\n",
    "gru = GRUClassifier(input_size, hidden_size, output_size, n_layers)\n",
    "\n",
    "loss_fn_gru = torch.nn.CrossEntropyLoss() # SHOULD USE WEIGHT PARAMETER SINCE UNBALANCED\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_gru = torch.optim.Adam(gru.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "567b04fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.149\n",
      "[2,  2000] loss: 0.080\n",
      "[3,  2000] loss: 0.059\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "num_epochs = 3\n",
    "gru = train_model(gru, loss_fn_gru, optimizer_gru, num_epochs, train_dataloader_2, val_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80696a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"GRU Metrics\")\n",
    "print(\"Train\")\n",
    "preds_gru_train = evaluate_model_metrics(gru, 2, train_dataloader_2)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_gru = evaluate_model_metrics(gru, 2, test_dataloader_2)\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c83d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742021ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "320089a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "GRU Metrics\n",
      "Train\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[115083,    282,      0],\n",
      "        [ 19618,   1353,      0],\n",
      "        [  1166,   4687,      0]])\n",
      "F1-Score (Average) tensor(0.3384)\n",
      "F1-Score (each):\n",
      "Class  0 : tensor(0.9161)\n",
      "Class  1 : tensor(0.0991)\n",
      "Class  2 : tensor(0.)\n",
      "Test\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[24571,    61,     0],\n",
      "        [ 4302,   261,     0],\n",
      "        [  267,  1007,     0]])\n",
      "F1-Score (Average) tensor(0.3342)\n",
      "F1-Score (each):\n",
      "Class  0 : tensor(0.9139)\n",
      "Class  1 : tensor(0.0886)\n",
      "Class  2 : tensor(0.)\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # model parameters for instantiation\n",
    "# input_size = seq_arrays.shape[2]\n",
    "# hidden_size = 32\n",
    "# output_size = 3\n",
    "# n_layers = 1\n",
    "\n",
    "# #instantiate model\n",
    "# gru = GRUClassifier(input_size, hidden_size, output_size, n_layers)\n",
    "\n",
    "# loss_fn_gru = torch.nn.CrossEntropyLoss() # SHOULD USE WEIGHT PARAMETER SINCE UNBALANCED\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# optimizer_gru = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "\n",
    "# # train\n",
    "# num_epochs = 3\n",
    "# gru = train_model(gru, loss_fn_gru, optimizer_gru, num_epochs, train_dataloader_3, val_dataloader_3)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"GRU Metrics\")\n",
    "print(\"Train\")\n",
    "preds_gru_train = evaluate_model_metrics(gru, 3, train_dataloader_3)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_gru = evaluate_model_metrics(gru, 3, test_dataloader_3)\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6dc72f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Metrics\n",
      "Train\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[136334,      2],\n",
      "        [  5853,      0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n",
      "Test\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[29194,     1],\n",
      "        [ 1274,     0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8830971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Metrics\n",
      "Train\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[136336,      0],\n",
      "        [  5853,      0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n",
      "Test\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[29195,     0],\n",
      "        [ 1274,     0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"LSTM Metrics\")\n",
    "print(\"Train\")\n",
    "preds_lstm_train = evaluate_model_metrics(lstm, 2, train_dataloader_2)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_lstm = evaluate_model_metrics(lstm, 2)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a4383088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Metrics\n",
      "Train\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[133283,   3053],\n",
      "        [  1170,   4683]])\n",
      "Precision:  tensor(0.6054)\n",
      "Recall:  tensor(0.8001)\n",
      "F1-Score:  tensor(0.6892)\n",
      "Test\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[28553,   642],\n",
      "        [  259,  1015]])\n",
      "Precision:  tensor(0.6126)\n",
      "Recall:  tensor(0.7967)\n",
      "F1-Score:  tensor(0.6926)\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"GRU Metrics\")\n",
    "print(\"Train\")\n",
    "preds_gru_train = evaluate_model_metrics(gru, 2, train_dataloader_2)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_gru = evaluate_model_metrics(gru, 2, test_dataloader_2)\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b8183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2672b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95c291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c29dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1df88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1dd240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11416645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba463dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f6b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8fd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796067bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc5737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a075c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #labels = labels.long() # convert to expected target datatype (Long which is equivalent to int here)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, h = gru(inputs)\n",
    "        \n",
    "#         outputs = outputs[:,154,0:2]\n",
    "        loss = loss_fn_gru(outputs,labels.view(-1).long()) # do i need to fix what's in here (even necessary to have it)\n",
    "#         loss = loss_fn(outputs, labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
