{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd9558f",
   "metadata": {},
   "source": [
    "<!-- # Overview\n",
    "- Project\n",
    "    - data engineer \n",
    "        - keep acceptable sessions\n",
    "        - create prediction classes and eliminate end of each session\n",
    "        - make same input size for rnn\n",
    "        - add features if this is suitable\n",
    "    - code each rnn\n",
    "        - vanilla rnn\n",
    "        - lstm\n",
    "        - gru -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9df30d",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Load Data\n",
    "    - \n",
    "- Modelling\n",
    "    - Train-Test-Split\n",
    "    - RNN\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - LSTM\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - GRU\n",
    "        - Model Def\n",
    "        - Model Training\n",
    "    - (Transformer Model Def)\n",
    "        - Transfer Learning/Model Def\n",
    "        - Model Training\n",
    "        \n",
    "- Evaluation and Comparison of Models\n",
    "    - Numerical Evalutaion Metrics\n",
    "        - AUC and F1 Score\n",
    "       - (Plotting number results comparing each as parameters change)\n",
    "    - Plotted Evaluation Metrics\n",
    "        - ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d865010",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8176e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulating data\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "# Custom Metrics and Evaluation\n",
    "from metrics import bin_class_metrics, multiclass_metrics, evaluate_model_metrics, print_metrics\n",
    "from ModelDefinitions import RNNClassifier, LSTMClassifier, GRUClassifier\n",
    "from TrainFunctions import create_file, load_data, getDataloaders, train_model\n",
    "\n",
    "# handling time data\n",
    "import time # for counting time for something to run\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# handling files\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56f934",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef443d",
   "metadata": {},
   "source": [
    "####\n",
    "- https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "\n",
    "- There's one additional rule of thumb that helps for supervised learning problems. You can usually prevent over-fitting if you keep your number of neurons below:\n",
    "\n",
    "𝑁ℎ=𝑁𝑠(𝛼∗(𝑁𝑖+𝑁𝑜))\n",
    "\n",
    "- 𝑁𝑖 = number of input neurons.\n",
    "- 𝑁𝑜 = number of output neurons.\n",
    "- 𝑁𝑠 = number of samples in training data set.\n",
    "- 𝛼 = an arbitrary scaling factor usually 2-10.\n",
    "\n",
    "Guy says he geneerally uses 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04000357",
   "metadata": {},
   "source": [
    "#### https://www.reddit.com/r/MachineLearning/comments/4behuh/does_the_number_of_layers_in_an_lstm_network/\n",
    "Some discussion about what \"depth\" in recurrent architectures means. Downward skip-connections seem to be the most helpful, but in general skip connections are critical in deep recurrent networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c464fe3",
   "metadata": {},
   "source": [
    "#### \n",
    "- https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "- usually one hidden layer is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f9121",
   "metadata": {},
   "source": [
    "### This looks like best tutorial to follow so far (in an article)\n",
    "- https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "- https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks\n",
    "    - didn't look bad either\n",
    "- https://docs.wandb.ai/guides/integrations/pytorch\n",
    "    - logging gradients with wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b305bb5",
   "metadata": {},
   "source": [
    "### Youtube Tutorial - Just Like Best One up There\n",
    "- https://www.youtube.com/watch?v=1vGOQAel2yU&ab_channel=SungKim\n",
    "- packed sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1882",
   "metadata": {},
   "source": [
    "# Model Definitions\n",
    "#### Training Function\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
    "    - training with validation as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd12754",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "- https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54724b14",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "- https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6bd4d",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3a5a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# select GPU / CPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfc751",
   "metadata": {},
   "source": [
    "## Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b413350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "model_name = \"vanilla_rnn\"\n",
    "output_size = 3\n",
    "b = 64 # batch size\n",
    "h = 64 # hidden size\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_dataset, test_dataset = load_data(output_size)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = getDataloaders(train_dataset, test_dataset, b)\n",
    "\n",
    "class_weights=compute_class_weight('balanced',classes = np.unique(train_dataset[:][1]), y = train_dataset[:][1].numpy().reshape(-1))\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "path = create_file(model_name, output_size, hidden_size, batch_size)\n",
    "\n",
    "# instantiate model\n",
    "rnn = RNNClassifier(input_size, h, output_size, n_layers).to(device)\n",
    "rnn = nn.DataParallel(rnn)\n",
    "\n",
    "loss_fn_rnn = torch.nn.CrossEntropyLoss() # weight = class_weights\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "rnn_val_loss_tot = []\n",
    "rnn_train_loss_tot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c571a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/6] train_loss: 0.17850 valid_loss: 0.17192\n",
      "Validation loss decreased (inf --> 0.171924).  Saving model ...\n",
      "[1/6] train_loss: 0.17213 valid_loss: 0.17207\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[2/6] train_loss: 0.17240 valid_loss: 0.17223\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[3/6] train_loss: 0.17230 valid_loss: 0.17200\n",
      "EarlyStopping counter: 3 out of 2\n",
      "[4/6] train_loss: 0.17219 valid_loss: 0.17201\n",
      "EarlyStopping counter: 4 out of 2\n",
      "[5/6] train_loss: 0.17223 valid_loss: 0.17193\n",
      "EarlyStopping counter: 5 out of 2\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "num_epochs = 6\n",
    "rnn, rnn_train_loss, rnn_val_loss = train_model(rnn, loss_fn_rnn, optimizer_rnn, num_epochs, train_dataloader, val_dataloader, 2, path, device, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f04aa969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(rnn_train_loss)+1),rnn_train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(rnn_val_loss)+1),rnn_val_loss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = rnn_val_loss.index(min(rnn_val_loss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(rnn_train_loss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f78f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "RNN Metrics\n",
      "Train\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[124640,      0],\n",
      "        [  5361,      0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n",
      "Test\n",
      "Confusion Matrix (0 in Top Left): \n",
      "tensor([[38955,     0],\n",
      "        [ 1670,     0]])\n",
      "\n",
      "\n",
      "precision is nan (has been set to 0)\n",
      "recall is nan (has been set to 0)\n",
      "F1score is nan (has been set to 0)\n",
      "\n",
      "\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-Score:  0\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"RNN Metrics\")\n",
    "print(\"Train\")\n",
    "preds_rnn_train = evaluate_model_metrics(rnn, 2, train_dataloader)\n",
    "\n",
    "print(\"Test\")\n",
    "preds_rnn = evaluate_model_metrics(rnn, 2, test_dataloader)\n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e0acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133fb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8cc8f4a",
   "metadata": {},
   "source": [
    "#### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [32, 64, 128, 256]\n",
    "batch_size = [32, 64, 128, 256]\n",
    "\n",
    "\n",
    "output_size = 3\n",
    "\n",
    "# datasets\n",
    "data_path = \"./cleaned_data\"\n",
    "trainset_path = \"/train_dataset_{}.pt\".format(output_size)\n",
    "testset_path = \"/test_dataset_{}.pt\".format(output_size)\n",
    "train_dataset = torch.load(data_path + trainset_path)\n",
    "test_dataset = torch.load(data_path + testset_path)\n",
    "\n",
    "num_epochs = 15\n",
    "lstm_train_loss_tot = []\n",
    "\n",
    "for h in hidden_size:\n",
    "    for b in batch_size:\n",
    "        # dataloaders\n",
    "        train_dataloader, val_dataloader, test_dataloader = getDataloaders(train_dataset, test_dataset, b)\n",
    "        \n",
    "        # model parameters for instantiation\n",
    "        weights = torch.FloatTensor([0.05, 0.95])\n",
    "        path = \"./model_checkpoints/lstm/output{}/hidden={}&batch={}/checkpoint.pt\".format(output_size,h, b)\n",
    "        \n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "           # Create a new directory because it does not exist\n",
    "           os.makedirs(path)\n",
    "\n",
    "        #instantiate model\n",
    "        lstm = LSTMClassifier(input_size, h, output_size, n_layers)\n",
    "\n",
    "        loss_fn_lstm = torch.nn.CrossEntropyLoss() # SHOULD USE WEIGHT PARAMETER SINCE UNBALANCED\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        optimizer_lstm = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # train\n",
    "        lstm, lstm_train_loss, lstm_val_loss = train_model(lstm, loss_fn_lstm, optimizer_lstm, num_epochs, train_dataloader, val_dataloader, 2, path)\n",
    "        lstm_train_loss_tot.append(lstm_train_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84101b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "model_name = \"lstm\"\n",
    "output_size = 3\n",
    "b = 64 # batch size\n",
    "h = 64 # hidden size\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_dataset, test_dataset = load_data(output_size)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = getDataloaders(train_dataset, test_dataset, b)\n",
    "\n",
    "class_weights=compute_class_weight('balanced',classes = np.unique(train_dataset[:][1]), y = train_dataset[:][1].numpy().reshape(-1))\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "path = create_file(model_name, output_size, hidden_size, batch_size)\n",
    "\n",
    "# instantiate model\n",
    "lstm = LSTMClassifier(input_size, h, output_size, n_layers).to(device)\n",
    "lstm = nn.DataParallel(lstm)\n",
    "\n",
    "loss_fn_lstm = torch.nn.CrossEntropyLoss() # weight = class_weights\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "optimizer_lstm = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "lstm_val_loss_tot = []\n",
    "lstm_train_loss_tot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fe15fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0/10] train_loss: 0.04810 valid_loss: 0.04819\n",
      "Validation loss decreased (inf --> 0.048195).  Saving model ...\n",
      "[ 1/10] train_loss: 0.04610 valid_loss: 0.04873\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 2/10] train_loss: 0.04515 valid_loss: 0.04592\n",
      "Validation loss decreased (0.048195 --> 0.045922).  Saving model ...\n",
      "[ 3/10] train_loss: 0.04418 valid_loss: 0.04629\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 4/10] train_loss: 0.04438 valid_loss: 0.04671\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[ 5/10] train_loss: 0.04299 valid_loss: 0.04474\n",
      "Validation loss decreased (0.045922 --> 0.044741).  Saving model ...\n",
      "[ 6/10] train_loss: 0.04244 valid_loss: 0.04667\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 7/10] train_loss: 0.04176 valid_loss: 0.05949\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[ 8/10] train_loss: 0.04224 valid_loss: 0.04248\n",
      "Validation loss decreased (0.044741 --> 0.042485).  Saving model ...\n",
      "[ 9/10] train_loss: 0.04049 valid_loss: 0.04400\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "num_epochs = 5\n",
    "lstm, lstm_train_loss, lstm_val_loss = train_model(lstm, loss_fn_lstm, optimizer_lstm, num_epochs, train_dataloader, val_dataloader, 2, path, device, False)\n",
    "lstm_val_loss_tot.append(lstm_val_loss)\n",
    "lstm_train_loss_tot.append(lstm_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585991a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotLoss(gru_train_loss, gru_val_loss, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(gru, model_name, output_size, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56081",
   "metadata": {},
   "source": [
    "#### Train GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06191831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden_size: 32, Batch_size: 32\n",
      "[ 0/15] train_loss: 0.45058 valid_loss: 0.15823\n",
      "Validation loss decreased (inf --> 0.158235).  Saving model ...\n",
      "[ 1/15] train_loss: 0.14978 valid_loss: 0.13519\n",
      "Validation loss decreased (0.158235 --> 0.135190).  Saving model ...\n",
      "[ 2/15] train_loss: 0.13608 valid_loss: 0.12770\n",
      "Validation loss decreased (0.135190 --> 0.127696).  Saving model ...\n",
      "[ 3/15] train_loss: 0.12927 valid_loss: 0.11555\n",
      "Validation loss decreased (0.127696 --> 0.115553).  Saving model ...\n",
      "[ 4/15] train_loss: 0.10157 valid_loss: 0.09234\n",
      "Validation loss decreased (0.115553 --> 0.092343).  Saving model ...\n",
      "[ 5/15] train_loss: 0.09202 valid_loss: 0.08684\n",
      "Validation loss decreased (0.092343 --> 0.086837).  Saving model ...\n",
      "[ 6/15] train_loss: 0.08998 valid_loss: 0.08330\n",
      "Validation loss decreased (0.086837 --> 0.083297).  Saving model ...\n",
      "[ 7/15] train_loss: 0.08264 valid_loss: 0.07582\n",
      "Validation loss decreased (0.083297 --> 0.075815).  Saving model ...\n",
      "[ 8/15] train_loss: 0.07767 valid_loss: 0.07231\n",
      "Validation loss decreased (0.075815 --> 0.072310).  Saving model ...\n",
      "[ 9/15] train_loss: 0.07499 valid_loss: 0.07701\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[10/15] train_loss: 0.07421 valid_loss: 0.07089\n",
      "Validation loss decreased (0.072310 --> 0.070891).  Saving model ...\n",
      "[11/15] train_loss: 0.07191 valid_loss: 0.06857\n",
      "Validation loss decreased (0.070891 --> 0.068569).  Saving model ...\n",
      "[12/15] train_loss: 0.07149 valid_loss: 0.06711\n",
      "Validation loss decreased (0.068569 --> 0.067108).  Saving model ...\n",
      "[13/15] train_loss: 0.07067 valid_loss: 0.07325\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[14/15] train_loss: 0.07039 valid_loss: 0.06818\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 32, Batch_size: 64\n",
      "[ 0/15] train_loss: 0.47016 valid_loss: 0.15917\n",
      "Validation loss decreased (inf --> 0.159168).  Saving model ...\n",
      "[ 1/15] train_loss: 0.14323 valid_loss: 0.14822\n",
      "Validation loss decreased (0.159168 --> 0.148216).  Saving model ...\n",
      "[ 2/15] train_loss: 0.18218 valid_loss: 0.18648\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 3/15] train_loss: 0.15040 valid_loss: 0.13561\n",
      "Validation loss decreased (0.148216 --> 0.135607).  Saving model ...\n",
      "[ 4/15] train_loss: 0.12921 valid_loss: 0.12711\n",
      "Validation loss decreased (0.135607 --> 0.127108).  Saving model ...\n",
      "[ 5/15] train_loss: 0.12273 valid_loss: 0.11702\n",
      "Validation loss decreased (0.127108 --> 0.117021).  Saving model ...\n",
      "[ 6/15] train_loss: 0.10290 valid_loss: 0.09786\n",
      "Validation loss decreased (0.117021 --> 0.097863).  Saving model ...\n",
      "[ 7/15] train_loss: 0.09510 valid_loss: 0.09346\n",
      "Validation loss decreased (0.097863 --> 0.093462).  Saving model ...\n",
      "[ 8/15] train_loss: 0.09188 valid_loss: 0.09206\n",
      "Validation loss decreased (0.093462 --> 0.092058).  Saving model ...\n",
      "[ 9/15] train_loss: 0.08855 valid_loss: 0.09008\n",
      "Validation loss decreased (0.092058 --> 0.090078).  Saving model ...\n",
      "[10/15] train_loss: 0.08611 valid_loss: 0.08627\n",
      "Validation loss decreased (0.090078 --> 0.086273).  Saving model ...\n",
      "[11/15] train_loss: 0.08289 valid_loss: 0.08288\n",
      "Validation loss decreased (0.086273 --> 0.082882).  Saving model ...\n",
      "[12/15] train_loss: 0.07933 valid_loss: 0.08211\n",
      "Validation loss decreased (0.082882 --> 0.082112).  Saving model ...\n",
      "[13/15] train_loss: 0.07609 valid_loss: 0.07731\n",
      "Validation loss decreased (0.082112 --> 0.077309).  Saving model ...\n",
      "[14/15] train_loss: 0.07351 valid_loss: 0.09031\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 32, Batch_size: 128\n",
      "[ 0/15] train_loss: 0.73471 valid_loss: 0.25434\n",
      "Validation loss decreased (inf --> 0.254344).  Saving model ...\n",
      "[ 1/15] train_loss: 0.17984 valid_loss: 0.14819\n",
      "Validation loss decreased (0.254344 --> 0.148189).  Saving model ...\n",
      "[ 2/15] train_loss: 0.15013 valid_loss: 0.14466\n",
      "Validation loss decreased (0.148189 --> 0.144658).  Saving model ...\n",
      "[ 3/15] train_loss: 0.14087 valid_loss: 0.13614\n",
      "Validation loss decreased (0.144658 --> 0.136137).  Saving model ...\n",
      "[ 4/15] train_loss: 0.13694 valid_loss: 0.13499\n",
      "Validation loss decreased (0.136137 --> 0.134990).  Saving model ...\n",
      "[ 5/15] train_loss: 0.13268 valid_loss: 0.12615\n",
      "Validation loss decreased (0.134990 --> 0.126155).  Saving model ...\n",
      "[ 6/15] train_loss: 0.12789 valid_loss: 0.12240\n",
      "Validation loss decreased (0.126155 --> 0.122401).  Saving model ...\n",
      "[ 7/15] train_loss: 0.12270 valid_loss: 0.11518\n",
      "Validation loss decreased (0.122401 --> 0.115178).  Saving model ...\n",
      "[ 8/15] train_loss: 0.10991 valid_loss: 0.09049\n",
      "Validation loss decreased (0.115178 --> 0.090491).  Saving model ...\n",
      "[ 9/15] train_loss: 0.08833 valid_loss: 0.08007\n",
      "Validation loss decreased (0.090491 --> 0.080073).  Saving model ...\n",
      "[10/15] train_loss: 0.08129 valid_loss: 0.07838\n",
      "Validation loss decreased (0.080073 --> 0.078383).  Saving model ...\n",
      "[11/15] train_loss: 0.08099 valid_loss: 0.07389\n",
      "Validation loss decreased (0.078383 --> 0.073894).  Saving model ...\n",
      "[12/15] train_loss: 0.07605 valid_loss: 0.07347\n",
      "Validation loss decreased (0.073894 --> 0.073473).  Saving model ...\n",
      "[13/15] train_loss: 0.07573 valid_loss: 0.07079\n",
      "Validation loss decreased (0.073473 --> 0.070785).  Saving model ...\n",
      "[14/15] train_loss: 0.07292 valid_loss: 0.07824\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 32, Batch_size: 256\n",
      "[ 0/15] train_loss: 0.91876 valid_loss: 0.77107\n",
      "Validation loss decreased (inf --> 0.771072).  Saving model ...\n",
      "[ 1/15] train_loss: 0.32965 valid_loss: 0.17349\n",
      "Validation loss decreased (0.771072 --> 0.173494).  Saving model ...\n",
      "[ 2/15] train_loss: 0.16195 valid_loss: 0.14636\n",
      "Validation loss decreased (0.173494 --> 0.146356).  Saving model ...\n",
      "[ 3/15] train_loss: 0.14687 valid_loss: 0.14613\n",
      "Validation loss decreased (0.146356 --> 0.146125).  Saving model ...\n",
      "[ 4/15] train_loss: 0.14227 valid_loss: 0.13819\n",
      "Validation loss decreased (0.146125 --> 0.138192).  Saving model ...\n",
      "[ 5/15] train_loss: 0.13979 valid_loss: 0.13134\n",
      "Validation loss decreased (0.138192 --> 0.131343).  Saving model ...\n",
      "[ 6/15] train_loss: 0.13412 valid_loss: 0.13722\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 7/15] train_loss: 0.13493 valid_loss: 0.12994\n",
      "Validation loss decreased (0.131343 --> 0.129944).  Saving model ...\n",
      "[ 8/15] train_loss: 0.13279 valid_loss: 0.12720\n",
      "Validation loss decreased (0.129944 --> 0.127205).  Saving model ...\n",
      "[ 9/15] train_loss: 0.13053 valid_loss: 0.12659\n",
      "Validation loss decreased (0.127205 --> 0.126587).  Saving model ...\n",
      "[10/15] train_loss: 0.12971 valid_loss: 0.12490\n",
      "Validation loss decreased (0.126587 --> 0.124898).  Saving model ...\n",
      "[11/15] train_loss: 0.14362 valid_loss: 0.12896\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[12/15] train_loss: 0.12868 valid_loss: 0.12512\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[13/15] train_loss: 0.12679 valid_loss: 0.12208\n",
      "Validation loss decreased (0.124898 --> 0.122076).  Saving model ...\n",
      "[14/15] train_loss: 0.12500 valid_loss: 0.12054\n",
      "Validation loss decreased (0.122076 --> 0.120543).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 64, Batch_size: 32\n",
      "[ 0/15] train_loss: 0.45976 valid_loss: 0.15768\n",
      "Validation loss decreased (inf --> 0.157678).  Saving model ...\n",
      "[ 1/15] train_loss: 0.13590 valid_loss: 0.10473\n",
      "Validation loss decreased (0.157678 --> 0.104728).  Saving model ...\n",
      "[ 2/15] train_loss: 0.10918 valid_loss: 0.10533\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 3/15] train_loss: 0.09566 valid_loss: 0.10150\n",
      "Validation loss decreased (0.104728 --> 0.101501).  Saving model ...\n",
      "[ 4/15] train_loss: 0.09198 valid_loss: 0.09082\n",
      "Validation loss decreased (0.101501 --> 0.090820).  Saving model ...\n",
      "[ 5/15] train_loss: 0.17393 valid_loss: 0.09480\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 6/15] train_loss: 0.08174 valid_loss: 0.07729\n",
      "Validation loss decreased (0.090820 --> 0.077294).  Saving model ...\n",
      "[ 7/15] train_loss: 0.07341 valid_loss: 0.07957\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 8/15] train_loss: 0.06543 valid_loss: 0.06314\n",
      "Validation loss decreased (0.077294 --> 0.063138).  Saving model ...\n",
      "[ 9/15] train_loss: 0.06410 valid_loss: 0.06363\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[10/15] train_loss: 0.05891 valid_loss: 0.06172\n",
      "Validation loss decreased (0.063138 --> 0.061720).  Saving model ...\n",
      "[11/15] train_loss: 0.05717 valid_loss: 0.06063\n",
      "Validation loss decreased (0.061720 --> 0.060629).  Saving model ...\n",
      "[12/15] train_loss: 0.05882 valid_loss: 0.05921\n",
      "Validation loss decreased (0.060629 --> 0.059205).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/15] train_loss: 0.04955 valid_loss: 0.05024\n",
      "Validation loss decreased (0.059205 --> 0.050235).  Saving model ...\n",
      "[14/15] train_loss: 0.04516 valid_loss: 0.04529\n",
      "Validation loss decreased (0.050235 --> 0.045290).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 64, Batch_size: 64\n",
      "[ 0/15] train_loss: 0.57805 valid_loss: 0.22175\n",
      "Validation loss decreased (inf --> 0.221746).  Saving model ...\n",
      "[ 1/15] train_loss: 0.17030 valid_loss: 0.13525\n",
      "Validation loss decreased (0.221746 --> 0.135252).  Saving model ...\n",
      "[ 2/15] train_loss: 0.11339 valid_loss: 0.10278\n",
      "Validation loss decreased (0.135252 --> 0.102781).  Saving model ...\n",
      "[ 3/15] train_loss: 0.09461 valid_loss: 0.09082\n",
      "Validation loss decreased (0.102781 --> 0.090822).  Saving model ...\n",
      "[ 4/15] train_loss: 0.08829 valid_loss: 0.08686\n",
      "Validation loss decreased (0.090822 --> 0.086864).  Saving model ...\n",
      "[ 5/15] train_loss: 0.08318 valid_loss: 0.07913\n",
      "Validation loss decreased (0.086864 --> 0.079130).  Saving model ...\n",
      "[ 6/15] train_loss: 0.07767 valid_loss: 0.07858\n",
      "Validation loss decreased (0.079130 --> 0.078576).  Saving model ...\n",
      "[ 7/15] train_loss: 0.07892 valid_loss: 0.07635\n",
      "Validation loss decreased (0.078576 --> 0.076353).  Saving model ...\n",
      "[ 8/15] train_loss: 0.07339 valid_loss: 0.07301\n",
      "Validation loss decreased (0.076353 --> 0.073013).  Saving model ...\n",
      "[ 9/15] train_loss: 0.07334 valid_loss: 0.07308\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[10/15] train_loss: 0.07471 valid_loss: 0.07084\n",
      "Validation loss decreased (0.073013 --> 0.070844).  Saving model ...\n",
      "[11/15] train_loss: 0.06926 valid_loss: 0.06978\n",
      "Validation loss decreased (0.070844 --> 0.069777).  Saving model ...\n",
      "[12/15] train_loss: 0.06863 valid_loss: 0.06998\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[13/15] train_loss: 0.06698 valid_loss: 0.06923\n",
      "Validation loss decreased (0.069777 --> 0.069230).  Saving model ...\n",
      "[14/15] train_loss: 0.59391 valid_loss: 0.65566\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 64, Batch_size: 128\n",
      "[ 0/15] train_loss: 0.53589 valid_loss: 0.19980\n",
      "Validation loss decreased (inf --> 0.199803).  Saving model ...\n",
      "[ 1/15] train_loss: 0.15980 valid_loss: 0.14451\n",
      "Validation loss decreased (0.199803 --> 0.144513).  Saving model ...\n",
      "[ 2/15] train_loss: 0.14062 valid_loss: 0.15071\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 3/15] train_loss: 0.13095 valid_loss: 0.12751\n",
      "Validation loss decreased (0.144513 --> 0.127509).  Saving model ...\n",
      "[ 4/15] train_loss: 0.12409 valid_loss: 0.12073\n",
      "Validation loss decreased (0.127509 --> 0.120734).  Saving model ...\n",
      "[ 5/15] train_loss: 0.10187 valid_loss: 0.08717\n",
      "Validation loss decreased (0.120734 --> 0.087168).  Saving model ...\n",
      "[ 6/15] train_loss: 0.08067 valid_loss: 0.07890\n",
      "Validation loss decreased (0.087168 --> 0.078901).  Saving model ...\n",
      "[ 7/15] train_loss: 0.07524 valid_loss: 0.07408\n",
      "Validation loss decreased (0.078901 --> 0.074085).  Saving model ...\n",
      "[ 8/15] train_loss: 0.07348 valid_loss: 0.07275\n",
      "Validation loss decreased (0.074085 --> 0.072748).  Saving model ...\n",
      "[ 9/15] train_loss: 0.06926 valid_loss: 0.06993\n",
      "Validation loss decreased (0.072748 --> 0.069935).  Saving model ...\n",
      "[10/15] train_loss: 0.06550 valid_loss: 0.06385\n",
      "Validation loss decreased (0.069935 --> 0.063847).  Saving model ...\n",
      "[11/15] train_loss: 0.06111 valid_loss: 0.06137\n",
      "Validation loss decreased (0.063847 --> 0.061371).  Saving model ...\n",
      "[12/15] train_loss: 0.05729 valid_loss: 0.05528\n",
      "Validation loss decreased (0.061371 --> 0.055281).  Saving model ...\n",
      "[13/15] train_loss: 0.05482 valid_loss: 0.05944\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[14/15] train_loss: 0.05133 valid_loss: 0.05133\n",
      "Validation loss decreased (0.055281 --> 0.051327).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 64, Batch_size: 256\n",
      "[ 0/15] train_loss: 0.93124 valid_loss: 0.78408\n",
      "Validation loss decreased (inf --> 0.784083).  Saving model ...\n",
      "[ 1/15] train_loss: 0.70904 valid_loss: 0.65644\n",
      "Validation loss decreased (0.784083 --> 0.656441).  Saving model ...\n",
      "[ 2/15] train_loss: 0.52168 valid_loss: 0.32242\n",
      "Validation loss decreased (0.656441 --> 0.322416).  Saving model ...\n",
      "[ 3/15] train_loss: 0.25972 valid_loss: 0.21817\n",
      "Validation loss decreased (0.322416 --> 0.218175).  Saving model ...\n",
      "[ 4/15] train_loss: 0.19518 valid_loss: 0.17857\n",
      "Validation loss decreased (0.218175 --> 0.178575).  Saving model ...\n",
      "[ 5/15] train_loss: 0.15195 valid_loss: 0.12661\n",
      "Validation loss decreased (0.178575 --> 0.126610).  Saving model ...\n",
      "[ 6/15] train_loss: 0.11889 valid_loss: 0.11174\n",
      "Validation loss decreased (0.126610 --> 0.111737).  Saving model ...\n",
      "[ 7/15] train_loss: 0.10776 valid_loss: 0.10053\n",
      "Validation loss decreased (0.111737 --> 0.100529).  Saving model ...\n",
      "[ 8/15] train_loss: 0.09898 valid_loss: 0.09067\n",
      "Validation loss decreased (0.100529 --> 0.090670).  Saving model ...\n",
      "[ 9/15] train_loss: 0.08879 valid_loss: 0.07864\n",
      "Validation loss decreased (0.090670 --> 0.078639).  Saving model ...\n",
      "[10/15] train_loss: 0.07857 valid_loss: 0.07320\n",
      "Validation loss decreased (0.078639 --> 0.073201).  Saving model ...\n",
      "[11/15] train_loss: 0.07548 valid_loss: 0.07138\n",
      "Validation loss decreased (0.073201 --> 0.071381).  Saving model ...\n",
      "[12/15] train_loss: 0.07846 valid_loss: 0.07065\n",
      "Validation loss decreased (0.071381 --> 0.070647).  Saving model ...\n",
      "[13/15] train_loss: 0.07399 valid_loss: 0.06917\n",
      "Validation loss decreased (0.070647 --> 0.069174).  Saving model ...\n",
      "[14/15] train_loss: 0.07277 valid_loss: 0.07310\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 128, Batch_size: 32\n",
      "[ 0/15] train_loss: 0.52011 valid_loss: 0.16093\n",
      "Validation loss decreased (inf --> 0.160929).  Saving model ...\n",
      "[ 1/15] train_loss: 0.13426 valid_loss: 0.12220\n",
      "Validation loss decreased (0.160929 --> 0.122199).  Saving model ...\n",
      "[ 2/15] train_loss: 0.10484 valid_loss: 0.08741\n",
      "Validation loss decreased (0.122199 --> 0.087405).  Saving model ...\n",
      "[ 3/15] train_loss: 0.08569 valid_loss: 0.07708\n",
      "Validation loss decreased (0.087405 --> 0.077078).  Saving model ...\n",
      "[ 4/15] train_loss: 0.07174 valid_loss: 0.06458\n",
      "Validation loss decreased (0.077078 --> 0.064578).  Saving model ...\n",
      "[ 5/15] train_loss: 0.15965 valid_loss: 0.13621\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 6/15] train_loss: 0.13266 valid_loss: 0.12561\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[ 7/15] train_loss: 0.11211 valid_loss: 0.09977\n",
      "EarlyStopping counter: 3 out of 2\n",
      "[ 8/15] train_loss: 0.26327 valid_loss: 0.49960\n",
      "EarlyStopping counter: 4 out of 2\n",
      "[ 9/15] train_loss: 0.42021 valid_loss: 0.31106\n",
      "EarlyStopping counter: 5 out of 2\n",
      "[10/15] train_loss: 0.22339 valid_loss: 0.22013\n",
      "EarlyStopping counter: 6 out of 2\n",
      "[11/15] train_loss: 0.13817 valid_loss: 0.09013\n",
      "EarlyStopping counter: 7 out of 2\n",
      "[12/15] train_loss: 0.07283 valid_loss: 0.05826\n",
      "Validation loss decreased (0.064578 --> 0.058260).  Saving model ...\n",
      "[13/15] train_loss: 0.05951 valid_loss: 0.05168\n",
      "Validation loss decreased (0.058260 --> 0.051680).  Saving model ...\n",
      "[14/15] train_loss: 0.05171 valid_loss: 0.04960\n",
      "Validation loss decreased (0.051680 --> 0.049595).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 128, Batch_size: 64\n",
      "[ 0/15] train_loss: 0.60097 valid_loss: 0.22214\n",
      "Validation loss decreased (inf --> 0.222145).  Saving model ...\n",
      "[ 1/15] train_loss: 0.17061 valid_loss: 0.13944\n",
      "Validation loss decreased (0.222145 --> 0.139442).  Saving model ...\n",
      "[ 2/15] train_loss: 0.13103 valid_loss: 0.12510\n",
      "Validation loss decreased (0.139442 --> 0.125105).  Saving model ...\n",
      "[ 3/15] train_loss: 0.11477 valid_loss: 0.09509\n",
      "Validation loss decreased (0.125105 --> 0.095087).  Saving model ...\n",
      "[ 4/15] train_loss: 0.09167 valid_loss: 0.08968\n",
      "Validation loss decreased (0.095087 --> 0.089675).  Saving model ...\n",
      "[ 5/15] train_loss: 0.08700 valid_loss: 0.08873\n",
      "Validation loss decreased (0.089675 --> 0.088731).  Saving model ...\n",
      "[ 6/15] train_loss: 0.08433 valid_loss: 0.08444\n",
      "Validation loss decreased (0.088731 --> 0.084444).  Saving model ...\n",
      "[ 7/15] train_loss: 0.08013 valid_loss: 0.07871\n",
      "Validation loss decreased (0.084444 --> 0.078710).  Saving model ...\n",
      "[ 8/15] train_loss: 0.07152 valid_loss: 0.06927\n",
      "Validation loss decreased (0.078710 --> 0.069269).  Saving model ...\n",
      "[ 9/15] train_loss: 0.06801 valid_loss: 0.07004\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[10/15] train_loss: 0.06665 valid_loss: 0.06828\n",
      "Validation loss decreased (0.069269 --> 0.068279).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/15] train_loss: 0.06762 valid_loss: 0.06903\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[12/15] train_loss: 0.06326 valid_loss: 0.06103\n",
      "Validation loss decreased (0.068279 --> 0.061026).  Saving model ...\n",
      "[13/15] train_loss: 0.07480 valid_loss: 0.12177\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[14/15] train_loss: 0.09244 valid_loss: 0.08006\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Finished Training\n",
      "Hidden_size: 128, Batch_size: 128\n",
      "[ 0/15] train_loss: 0.80905 valid_loss: 0.64453\n",
      "Validation loss decreased (inf --> 0.644526).  Saving model ...\n",
      "[ 1/15] train_loss: 0.32721 valid_loss: 0.21451\n",
      "Validation loss decreased (0.644526 --> 0.214511).  Saving model ...\n",
      "[ 2/15] train_loss: 0.17814 valid_loss: 0.15217\n",
      "Validation loss decreased (0.214511 --> 0.152174).  Saving model ...\n",
      "[ 3/15] train_loss: 0.12423 valid_loss: 0.10216\n",
      "Validation loss decreased (0.152174 --> 0.102155).  Saving model ...\n",
      "[ 4/15] train_loss: 0.10147 valid_loss: 0.09178\n",
      "Validation loss decreased (0.102155 --> 0.091775).  Saving model ...\n",
      "[ 5/15] train_loss: 0.09199 valid_loss: 0.08851\n",
      "Validation loss decreased (0.091775 --> 0.088505).  Saving model ...\n",
      "[ 6/15] train_loss: 0.08697 valid_loss: 0.08334\n",
      "Validation loss decreased (0.088505 --> 0.083341).  Saving model ...\n",
      "[ 7/15] train_loss: 0.08052 valid_loss: 0.07933\n",
      "Validation loss decreased (0.083341 --> 0.079334).  Saving model ...\n",
      "[ 8/15] train_loss: 0.07494 valid_loss: 0.07083\n",
      "Validation loss decreased (0.079334 --> 0.070830).  Saving model ...\n",
      "[ 9/15] train_loss: 0.07103 valid_loss: 0.06694\n",
      "Validation loss decreased (0.070830 --> 0.066937).  Saving model ...\n",
      "[10/15] train_loss: 0.06928 valid_loss: 0.06426\n",
      "Validation loss decreased (0.066937 --> 0.064264).  Saving model ...\n",
      "[11/15] train_loss: 0.06776 valid_loss: 0.07967\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[12/15] train_loss: 0.06663 valid_loss: 0.06333\n",
      "Validation loss decreased (0.064264 --> 0.063332).  Saving model ...\n",
      "[13/15] train_loss: 0.06612 valid_loss: 0.06372\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[14/15] train_loss: 0.05924 valid_loss: 0.05479\n",
      "Validation loss decreased (0.063332 --> 0.054792).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 128, Batch_size: 256\n",
      "[ 0/15] train_loss: 0.56697 valid_loss: 0.17403\n",
      "Validation loss decreased (inf --> 0.174027).  Saving model ...\n",
      "[ 1/15] train_loss: 0.15830 valid_loss: 0.13846\n",
      "Validation loss decreased (0.174027 --> 0.138459).  Saving model ...\n",
      "[ 2/15] train_loss: 0.13400 valid_loss: 0.13262\n",
      "Validation loss decreased (0.138459 --> 0.132616).  Saving model ...\n",
      "[ 3/15] train_loss: 0.13191 valid_loss: 0.12320\n",
      "Validation loss decreased (0.132616 --> 0.123198).  Saving model ...\n",
      "[ 4/15] train_loss: 0.10088 valid_loss: 0.10384\n",
      "Validation loss decreased (0.123198 --> 0.103839).  Saving model ...\n",
      "[ 5/15] train_loss: 0.08433 valid_loss: 0.07921\n",
      "Validation loss decreased (0.103839 --> 0.079205).  Saving model ...\n",
      "[ 6/15] train_loss: 0.08148 valid_loss: 0.07891\n",
      "Validation loss decreased (0.079205 --> 0.078909).  Saving model ...\n",
      "[ 7/15] train_loss: 0.08688 valid_loss: 0.08274\n",
      "EarlyStopping counter: 1 out of 2\n",
      "[ 8/15] train_loss: 0.09922 valid_loss: 0.08323\n",
      "EarlyStopping counter: 2 out of 2\n",
      "[ 9/15] train_loss: 0.07715 valid_loss: 0.07913\n",
      "EarlyStopping counter: 3 out of 2\n",
      "[10/15] train_loss: 0.07278 valid_loss: 0.07094\n",
      "Validation loss decreased (0.078909 --> 0.070939).  Saving model ...\n",
      "[11/15] train_loss: 0.06651 valid_loss: 0.06312\n",
      "Validation loss decreased (0.070939 --> 0.063121).  Saving model ...\n",
      "[12/15] train_loss: 0.06245 valid_loss: 0.06055\n",
      "Validation loss decreased (0.063121 --> 0.060549).  Saving model ...\n",
      "[13/15] train_loss: 0.05968 valid_loss: 0.05846\n",
      "Validation loss decreased (0.060549 --> 0.058460).  Saving model ...\n",
      "[14/15] train_loss: 0.05686 valid_loss: 0.05707\n",
      "Validation loss decreased (0.058460 --> 0.057073).  Saving model ...\n",
      "Finished Training\n",
      "Hidden_size: 256, Batch_size: 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f5639b7bd0f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mgru_train_loss_tot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-b8182cf3d405>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, num_epochs, train_dataloader, val_dataloader, patience, path, load_best)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do i need to fix what's in here (even necessary to have it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = [64, 128, 256]\n",
    "batch_size = [32, 64, 128, 256]\n",
    "\n",
    "output_size = 3\n",
    "\n",
    "# datasets\n",
    "data_path = \"./cleaned_data\"\n",
    "trainset_path = \"/train_dataset_{}.pt\".format(output_size)\n",
    "testset_path = \"/test_dataset_{}.pt\".format(output_size)\n",
    "train_dataset = torch.load(data_path + trainset_path)\n",
    "test_dataset = torch.load(data_path + testset_path)\n",
    "\n",
    "num_epochs = 40\n",
    "gru_val_loss_tot = []\n",
    "\n",
    "for h in hidden_size:\n",
    "    for b in batch_size:\n",
    "        print(\"Hidden_size: {}, Batch_size: {}\".format(h, b))\n",
    "      \n",
    "        # dataloaders\n",
    "        train_dataloader, val_dataloader, test_dataloader = getDataloaders(train_dataset, test_dataset, b)\n",
    "        \n",
    "        # model parameters for instantiation\n",
    "        weights = torch.FloatTensor([0.95, 0.05])\n",
    "        \n",
    "        dir_path = \"./model_checkpoints/gru/output{}/hidden={}&batch={}\".format(output_size, h, b)\n",
    "        isExist = os.path.exists(dir_path)\n",
    "        if not isExist:\n",
    "           # Create a new directory because it does not exist\n",
    "           os.makedirs(dir_path)\n",
    "            \n",
    "        f = open(dir_path + \"/checkpoint.pt\", 'w')\n",
    "        f.close()\n",
    "        \n",
    "        path = dir_path + \"/checkpoint.pt\"\n",
    "\n",
    "        #instantiate model\n",
    "        gru = GRUClassifier(input_size, h, output_size, n_layers)\n",
    "\n",
    "        loss_fn_gru = torch.nn.CrossEntropyLoss(weights)\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        optimizer_gru = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # train\n",
    "        gru, gru_train_loss, gru_val_loss = train_model(gru, loss_fn_gru, optimizer_gru, num_epochs, train_dataloader, val_dataloader, 2, path)\n",
    "        gru_val_loss_tot.append(gru_train_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = [32, 32, 32, 32, 64, 64, 64, 64, 128, 128, 128, 128]\n",
    "bat = [32, 64,128, 256, 32, 64,128, 256, 32, 64,128, 256]\n",
    "output_size = 3\n",
    "\n",
    "\n",
    "for i, h in enumerate(hid):\n",
    "    print(\"Hidden: {}, Batch: {}\".format(h, bat[i]))\n",
    "    path = \"./model_checkpoints/gru/output{}/hidden={}&batch={}/checkpoint.pt\".format(output_size, h, bat[i])\n",
    "    gru = GRUClassifier(input_size, h, output_size, n_layers)\n",
    "    gru.load_state_dict(torch.load(path))\n",
    "    preds_gru = evaluate_model_metrics(gru, output_size, test_dataloader)\n",
    "    print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4479e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34177568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a48919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which problem to solve\n",
    "output_size = 3\n",
    "\n",
    "# get datasets\n",
    "train_dataset, test_dataset = load_data(output_size)\n",
    "\n",
    "# parameters\n",
    "input_size = len(train_dataset[0][0][0])\n",
    "n_layers = 1\n",
    "model_name = \"gru\"\n",
    "b = 64 # batch size\n",
    "h = 64 # hidden size\n",
    "learning_rate = 0.001\n",
    "\n",
    "# get class weights for imbalanced datasets\n",
    "y = train_dataset[:][1]\n",
    "class_weights=compute_class_weight('balanced',classes = np.unique(y), y = y.numpy().reshape(-1))\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# create path to store checkpoints\n",
    "path = create_file(model_name, output_size, h, b)\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader, val_dataloader, test_dataloader = getDataloaders(train_dataset, test_dataset, b)\n",
    "\n",
    "# instantiate model\n",
    "gru = GRUClassifier(input_size, h, output_size, n_layers).to(device)\n",
    "gru = nn.DataParallel(gru)\n",
    "\n",
    "loss_fn_gru = torch.nn.CrossEntropyLoss() # weight = class_weights\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "optimizer_gru = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "\n",
    "gru_val_loss_tot = []\n",
    "gru_train_loss_tot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567b04fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f6af051f147b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgru_val_loss_tot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgru_train_loss_tot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PythonJupyter/ShopperIntentPredictionClickstream/TrainFunctions.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, num_epochs, train_dataloader, val_dataloader, patience, path, device, load_best)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mvalid_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mavg_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# train\n",
    "num_epochs = 6\n",
    "gru, gru_train_loss, gru_val_loss = train_model(gru, loss_fn_gru, optimizer_gru, num_epochs, train_dataloader, val_dataloader, 2, path, device, False)\n",
    "gru_val_loss_tot.append(gru_val_loss)\n",
    "gru_train_loss_tot.append(gru_train_loss)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotLoss(gru_train_loss, gru_val_loss, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8892dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(gru, model_name, output_size, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e335545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fcdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742021ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b8183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2672b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95c291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c29dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1df88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1dd240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11416645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba463dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f6b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8fd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796067bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc5737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a075c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #labels = labels.long() # convert to expected target datatype (Long which is equivalent to int here)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, h = gru(inputs)\n",
    "        \n",
    "#         outputs = outputs[:,154,0:2]\n",
    "        loss = loss_fn_gru(outputs,labels.view(-1).long()) # do i need to fix what's in here (even necessary to have it)\n",
    "#         loss = loss_fn(outputs, labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
